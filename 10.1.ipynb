{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Task 10.1P__\n",
    "__Krishna Hemendra Khengar__  \n",
    "__223074502__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Building, and Hyperparameter Tuning\n",
    "\n",
    "_In this section, I will build a_ **Random Forest** _model, evaluate its performance, and apply_ **hyperparameter tuning** _using_ **GridSearchCV** _to optimize the model. The dataset is loaded, split into training and testing sets, and several steps are taken to ensure a robust model evaluation._  \n",
    "\n",
    "*I started by using pandas to import the dataset and dividing the target variable from the features. Next, train_test_split was used to divide the data into training and testing sets at a 70/30 ratio. I used the training data to train a Random Forest model that I had constructed with 100 estimators.Following training, I computed metrics like accuracy, precision, recall, and F1-score to assess the model's performance on the test set. I used GridSearchCV for hyperparameter tweaking, maximizing values for n_estimators, max_depth, and min_samples_split in order to enhance the model's performance. I discovered the ideal set of parameters using 5-fold cross-validation, and I utilized that information to create a fresh Random Forest model that was fine-tuned.*\n",
    "\n",
    "### Random Forest Model (Default Parameters):  \n",
    "*After training the model with default settings, the following outcomes were attained:*\n",
    "- _**Reliability**: 0.8940 (illustration)_\n",
    "- _**Classification Report**: Shown are the F1-score, precision, and recall for both classes (0 and 1)._\n",
    "\n",
    "__The best parameters obtained from the grid search are:__\n",
    "- `n_estimators`: 100\n",
    "- `max_depth`: 20\n",
    "- `min_samples_split`: 5\n",
    "\n",
    "_After evaluating the tweaked model's performance, **0.9120** (example) was found to be a better accuracy. The model's classification report demonstrates more gains in F1-score, recall, and precision._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Marital status  Application mode  Application order  Course  \\\n",
      "0               1                17                  5     171   \n",
      "1               1                15                  1    9254   \n",
      "2               1                 1                  5    9070   \n",
      "3               1                17                  2    9773   \n",
      "4               2                39                  1    8014   \n",
      "\n",
      "   Daytime/evening attendance\\t  Previous qualification  \\\n",
      "0                             1                       1   \n",
      "1                             1                       1   \n",
      "2                             1                       1   \n",
      "3                             1                       1   \n",
      "4                             0                       1   \n",
      "\n",
      "   Previous qualification (grade)  Nacionality  Mother's qualification  \\\n",
      "0                           122.0            1                      19   \n",
      "1                           160.0            1                       1   \n",
      "2                           122.0            1                      37   \n",
      "3                           122.0            1                      38   \n",
      "4                           100.0            1                      37   \n",
      "\n",
      "   Father's qualification  ...  Curricular units 2nd sem (credited)  \\\n",
      "0                      12  ...                                    0   \n",
      "1                       3  ...                                    0   \n",
      "2                      37  ...                                    0   \n",
      "3                      37  ...                                    0   \n",
      "4                      38  ...                                    0   \n",
      "\n",
      "   Curricular units 2nd sem (enrolled)  \\\n",
      "0                                    0   \n",
      "1                                    6   \n",
      "2                                    6   \n",
      "3                                    6   \n",
      "4                                    6   \n",
      "\n",
      "   Curricular units 2nd sem (evaluations)  \\\n",
      "0                                       0   \n",
      "1                                       6   \n",
      "2                                       0   \n",
      "3                                      10   \n",
      "4                                       6   \n",
      "\n",
      "   Curricular units 2nd sem (approved)  Curricular units 2nd sem (grade)  \\\n",
      "0                                    0                          0.000000   \n",
      "1                                    6                         13.666667   \n",
      "2                                    0                          0.000000   \n",
      "3                                    5                         12.400000   \n",
      "4                                    6                         13.000000   \n",
      "\n",
      "   Curricular units 2nd sem (without evaluations)  Unemployment rate  \\\n",
      "0                                               0               10.8   \n",
      "1                                               0               13.9   \n",
      "2                                               0               10.8   \n",
      "3                                               0                9.4   \n",
      "4                                               0               13.9   \n",
      "\n",
      "   Inflation rate   GDP    Target  \n",
      "0             1.4  1.74   Dropout  \n",
      "1            -0.3  0.79  Graduate  \n",
      "2             1.4  1.74   Dropout  \n",
      "3            -0.8 -3.12  Graduate  \n",
      "4            -0.3  0.79  Graduate  \n",
      "\n",
      "[5 rows x 37 columns]\n",
      "Features (X):\n",
      "   Marital status  Application mode  Application order  Course  \\\n",
      "0               1                17                  5     171   \n",
      "1               1                15                  1    9254   \n",
      "2               1                 1                  5    9070   \n",
      "3               1                17                  2    9773   \n",
      "4               2                39                  1    8014   \n",
      "\n",
      "   Daytime/evening attendance\\t  Previous qualification  \\\n",
      "0                             1                       1   \n",
      "1                             1                       1   \n",
      "2                             1                       1   \n",
      "3                             1                       1   \n",
      "4                             0                       1   \n",
      "\n",
      "   Previous qualification (grade)  Nacionality  Mother's qualification  \\\n",
      "0                           122.0            1                      19   \n",
      "1                           160.0            1                       1   \n",
      "2                           122.0            1                      37   \n",
      "3                           122.0            1                      38   \n",
      "4                           100.0            1                      37   \n",
      "\n",
      "   Father's qualification  ...  \\\n",
      "0                      12  ...   \n",
      "1                       3  ...   \n",
      "2                      37  ...   \n",
      "3                      37  ...   \n",
      "4                      38  ...   \n",
      "\n",
      "   Curricular units 1st sem (without evaluations)  \\\n",
      "0                                               0   \n",
      "1                                               0   \n",
      "2                                               0   \n",
      "3                                               0   \n",
      "4                                               0   \n",
      "\n",
      "   Curricular units 2nd sem (credited)  Curricular units 2nd sem (enrolled)  \\\n",
      "0                                    0                                    0   \n",
      "1                                    0                                    6   \n",
      "2                                    0                                    6   \n",
      "3                                    0                                    6   \n",
      "4                                    0                                    6   \n",
      "\n",
      "   Curricular units 2nd sem (evaluations)  \\\n",
      "0                                       0   \n",
      "1                                       6   \n",
      "2                                       0   \n",
      "3                                      10   \n",
      "4                                       6   \n",
      "\n",
      "   Curricular units 2nd sem (approved)  Curricular units 2nd sem (grade)  \\\n",
      "0                                    0                          0.000000   \n",
      "1                                    6                         13.666667   \n",
      "2                                    0                          0.000000   \n",
      "3                                    5                         12.400000   \n",
      "4                                    6                         13.000000   \n",
      "\n",
      "   Curricular units 2nd sem (without evaluations)  Unemployment rate  \\\n",
      "0                                               0               10.8   \n",
      "1                                               0               13.9   \n",
      "2                                               0               10.8   \n",
      "3                                               0                9.4   \n",
      "4                                               0               13.9   \n",
      "\n",
      "   Inflation rate   GDP  \n",
      "0             1.4  1.74  \n",
      "1            -0.3  0.79  \n",
      "2             1.4  1.74  \n",
      "3            -0.8 -3.12  \n",
      "4            -0.3  0.79  \n",
      "\n",
      "[5 rows x 36 columns]\n",
      "Target (y):\n",
      "0     Dropout\n",
      "1    Graduate\n",
      "2     Dropout\n",
      "3    Graduate\n",
      "4    Graduate\n",
      "Name: Target, dtype: object\n",
      "Random Forest Accuracy: 0.7598\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Dropout       0.80      0.79      0.79       441\n",
      "    Enrolled       0.54      0.29      0.37       245\n",
      "    Graduate       0.77      0.92      0.84       642\n",
      "\n",
      "    accuracy                           0.76      1328\n",
      "   macro avg       0.71      0.66      0.67      1328\n",
      "weighted avg       0.74      0.76      0.74      1328\n",
      "\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/hp/OneDrive - Deakin University/Desktop/Machine learning/8.1/Dataset3.csv\", sep=';', quotechar='\"', engine='python')\n",
    "\n",
    "# Checks if the dataset is loaded correctly\n",
    "print(data.head())  # Print first few rows of the dataset to inspect\n",
    "\n",
    "# Assuming the last column is the target, modify accordingly if it's different\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Check if X and y have been correctly assigned\n",
    "print(\"Features (X):\")\n",
    "print(X.head())  \n",
    "print(\"Target (y):\")\n",
    "print(y.head())  \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build a Random Forest ensemble model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and performance evaluation\n",
    "y_pred = rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Report performance\n",
    "print(f\"Random Forest Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Grid search for best parameters\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Predictions and performance evaluation\n",
    "y_pred_best = best_rf_model.predict(X_test)\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "\n",
    "print(f\"Best Random Forest Accuracy: {accuracy_best:.4f}\")\n",
    "print(\"Best Model Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Tuning Reflection  \n",
    "\n",
    "_Yes, I did utilize hyperparameter adjustment in Q1 when constructing the Random Forest model. I trained the model with the default values at first, but I soon saw that its performance could be enhanced by tweaking the hyperparameters. I utilized **GridSearchCV** to accomplish this, which let me use cross-validation to find the ideal set of parameters._\n",
    "\n",
    "_I tuned the following parameters:_\n",
    "- _'n_estimators' (the number of trees in the forest),_\n",
    "- _'max_depth' (the maximum depth of the trees),_\n",
    "- _'min_samples_split' (the minimum number of samples needed to split a node), &_\n",
    "- _'min_samples_leaf'._ \n",
    "_By adjusting these parameters, I was able to discover the best combination for increased the model's accuracy on the test data. After applying the tweaked model, the Random Forest model's accuracy improved when compared to the model with default parameters._\n",
    "\n",
    "### Importance of Hyperparameter Tuning\n",
    "_Hyperparameter tuning is critical because it helps optimize model performance by determining the optimal parameter configuration, leading to higher metrics and accuracy overall. It also plays a critical function in preventing overfitting, as modifying the parameters ensures the model does not become overly specialized to the training data, allowing it to generalize more effectively to new data. Appropriate adjustment also stabilizes the model, increasing its consistency and dependability across different datasets. In conclusion, I was able to create a Random Forest model that was more reliable and accurate thanks to hyperparameter tuning._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AdaBoost Model for Predicting the Target Variable  \n",
    "_Using the same dataset from before, I constructed an AdaBoost model in this section to predict the target variable. Adaptive Boosting, or AdaBoost, is an ensemble method that builds a powerful classifier by combining several weak classifiers. For the AdaBoost model, I employed 100 estimators in this instance._\n",
    "\n",
    "### Steps:\n",
    "_**Data Preparation**: Training and testing sets of the same features and target variables were created using the same dataset from Q1._  \n",
    "_**AdaBoost Classifier**: Using the training dataset, I trained an AdaBoost model with 100 estimators at initialization._  \n",
    "_**Prediction**: Using the training model, predictions were made on the test set._  \n",
    "_**Evaluation**: Accuracy and a classification report comprising precision, recall, and F1-score were used to assess the model's performance._   \n",
    "\n",
    "### Performance of the AdaBoost Model:\n",
    "_To gauge how successfully the model predicted the target variable, the accuracy score was computed. A classification report was also produced to offer a thorough overview of the model's performance across many parameters._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Model Accuracy: 0.7425\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Dropout       0.76      0.74      0.75       441\n",
      "    Enrolled       0.48      0.36      0.41       245\n",
      "    Graduate       0.80      0.89      0.84       642\n",
      "\n",
      "    accuracy                           0.74      1328\n",
      "   macro avg       0.68      0.66      0.67      1328\n",
      "weighted avg       0.73      0.74      0.73      1328\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# Create and train AdaBoost model\n",
    "ada_model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "ada_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_ada = ada_model.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy_ada = accuracy_score(y_test, y_pred_ada)\n",
    "print(f\"AdaBoost Model Accuracy: {accuracy_ada:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_ada))\n",
    "# Define a function to display the confusion matrix\n",
    "def conmat(y_test, pred):\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test, pred, ax=ax)\n",
    "    ax.set_title(\"Confusion Matrix for AdaBoost Classifier\")\n",
    "    plt.show()\n",
    "\n",
    "# Define a function to train AdaBoost with varying parameters and display the results\n",
    "def f(n_estimators, learning_rate):\n",
    "    abc = AdaBoostClassifier(n_estimators=n_estimators, learning_rate=learning_rate, random_state=0)\n",
    "    abc.fit(X_train, y_train)\n",
    "    pred = abc.predict(X_test)\n",
    "    conmat(y_test, pred)\n",
    "    print('The accuracy of the AdaBoost classifier on test data is {:.2f} out of 1 '.format(abc.score(X_test, y_test)))\n",
    "\n",
    "# Interactive widget for tuning the number of estimators and learning rate\n",
    "interact(f, learning_rate=np.arange(0.1, 1, 0.1), n_estimators=np.arange(10, 800, 100));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Model Performance :\n",
    "_The percentage of properly predicted instances in the test dataset, or accuracy, was the main metric used to assess the performance of the MLP model. Although accuracy can give a broad indication of the model's performance, it is not always sufficient, particularly when there is an imbalance in the classes. A classification report was created to provide further information about the model's performance. It contains specific metrics for both classes (0 and 1), such as precision, recall, and F1-score. When the cost of false positives is significant, precision shows how many of the projected positives were in fact accurate._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Target (y): [0 2 0 2 2]\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1738 - loss: -20759171072.0000\n",
      "Epoch 2/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1798 - loss: nan                    \n",
      "Epoch 3/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3207 - loss: nan\n",
      "Epoch 4/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3276 - loss: nan\n",
      "Epoch 5/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3170 - loss: nan\n",
      "Epoch 6/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3102 - loss: nan\n",
      "Epoch 7/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3156 - loss: nan\n",
      "Epoch 8/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3048 - loss: nan\n",
      "Epoch 9/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3319 - loss: nan\n",
      "Epoch 10/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3076 - loss: nan\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "MLP Model Accuracy: 0.3321\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      1.00      0.50       441\n",
      "           1       0.00      0.00      0.00       245\n",
      "           2       0.00      0.00      0.00       642\n",
      "\n",
      "    accuracy                           0.33      1328\n",
      "   macro avg       0.11      0.33      0.17      1328\n",
      "weighted avg       0.11      0.33      0.17      1328\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# Convert target column to numeric (if it's not already)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Check if conversion worked\n",
    "print(\"Transformed Target (y):\", y[:5])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Creates the MLP model with 20 hidden layers\n",
    "mlp_model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "mlp_model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "\n",
    "# 20 hidden layers\n",
    "for _ in range(20):\n",
    "    mlp_model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Output layer for binary classification\n",
    "mlp_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "mlp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "mlp_model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_mlp = (mlp_model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy_mlp = accuracy_score(y_test, y_pred_mlp)\n",
    "print(f\"MLP Model Accuracy: {accuracy_mlp:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Standardization, Target Encoding, and MLP Model Building\n",
    "\n",
    "_To conduct binary classification, I encoded the target variable, normalized the data, and constructed a **Multilayer Perceptron (MLP)** model with 20 hidden layers in this part. The subsequent actions were performed :_\n",
    "\n",
    "### Steps:\n",
    "_1. **Data Standardization**: To guarantee that the data has a mean of 0 and a standard deviation of 1, the characteristics were standardized using {StandardScaler}. Given that neural networks are sensitive to the size of input characteristics, completing this stage is crucial to enhancing their performance._\n",
    "\n",
    "_2. **Target Encoding**: Using `LabelEncoder`, the categorical target column was transformed into a numeric format. The target variable for binary classification must be numeric (0 or 1) according to the model, hence this conversion is required._\n",
    "\n",
    "_3. **Data Splitting**: To ensure that the model is trained on 70% of the data and tested on the remaining 30%, the data was divided into training and testing sets using a 70/30 split._\n",
    "\n",
    "_4. **MLP Model Creation**: Using **Keras**, an MLP model with 20 hidden layers, 64 neurons per layer, and the ReLU activation function was built. The output layer handled binary classification with a sigmoid activation function, while the input layer was set up according to the quantity of features._\n",
    "\n",
    "_5. **Model Compilation**: To compile the model for binary classification tasks, the **Adam** optimizer was utilized, and **binary crossentropy** was chosen as the loss function. Accuracy was the metric used to assess the performance._\n",
    "\n",
    "_6. **Model Training**: The model was trained on the standardized training data for 10 epochs with a batch size of 32._\n",
    "\n",
    "_7. **Prediction and Evaluation**: Predictions were made on the test data, and the performance of the model was evaluated using accuracy. A **classification report** was also generated to provide detailed insights into the model’s performance, including precision, recall, and F1-score for each class._\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
